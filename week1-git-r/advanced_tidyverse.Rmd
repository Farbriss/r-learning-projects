---
title: "Advanced tidyverse Übungen"
author: "Fabrice Keller"
date: "`r Sys.Date()`"
output: pdf_document
---

Lernziele:

- Komplexe Datenmanipulationen mit dplyr
- Erweiterte Visualisierungen mit ggplot2
- Daten-Pivoting und String-Operationen
- Funktionale Programmierung Grundlagen


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Laden der nötigen Packages für diese Session
```{r}
library(tidyverse)       # Meta-Package: dplyr, ggplot2, tidyr, stringr etc.
library(palmerpenguins)  # Saubere Beispieldaten für Ökologie/Biologie
library(nycflights13)    # Flugdaten mit datetime, joins etc.
library(lubridate)       # Einfache Datums-/Zeitmanipulation
```
Überprüfe, welche Pakete geladen sind
```{r}
sessionInfo()
```

# Block 1: Fortgeschrittene dplyr-Operationen

## 1.1 Komplexe Mutate-Operationen

Konzept: `mutate()` erstellt neue Spalten basierend auf bestehenden Daten. Mit `case_when()` können wir komplexe bedingte Logik implementieren.

Bevor komplexe Analysen durchgeführt werden, ist eine gründliche Exploration des Datensatzes unerlässlich. Die Funktionen `glimpse()`, `summary()` und `head()` bieten dabei unterschiedliche, sich ergänzende Perspektiven auf die Datenstruktur: `glimpse()` zeigt kompakt alle Variablen mit ihren Datentypen und ersten Werten, `summary()` liefert statistische Kennzahlen für numerische Variablen und Häufigkeiten für kategorische Variablen, während `head()` die ersten Zeilen im ursprünglichen Tabellenformat anzeigt.
```{r}
glimpse(penguins)  # Zeigt Datentypen und erste Werte
summary(penguins)  # Statistische Übersicht
```
Die ersten Zeilen anschauen
```{r}
head(penguins, 10)
```

Der `palmerpenguins`-Datensatz eignet sich hervorragend für Lernzwecke, da er saubere, realistische biologische Daten ohne komplizierte Datenqualitätsprobleme enthält und gleichzeitig interessante biologische Fragestellungen ermöglicht. Die nachfolgende erweiterte `mutate()`-Operation zeigt die Vielseitigkeit dieser Funktion bei der Erstellung neuer Variablen durch verschiedene Transformationsarten.

Die bedingte Logik mit `case_when()` ermöglicht eine elegante Kategorisierung numerischer Werte in aussagekräftige Gruppen. Im Gegensatz zu verschachtelten `ifelse()`-Konstruktionen ist `case_when()` bei mehreren Bedingungen deutlich lesbarer und weniger fehleranfällig. Die Syntax folgt dem Muster `Bedingung ~ Ergebnis`, wobei die Bedingungen sequenziell geprüft werden und `TRUE ~ "Default"` als Auffangkategorie für alle nicht erfassten Fälle dient.

Numerische Berechnungen wie Verhältnisse (Ratios) sind in der biologischen Forschung besonders wertvoll, da sie allometrische Beziehungen zwischen Körperteilen aufdecken können. Das Verhältnis von Schnabellänge zu -tiefe (`bill_ratio`) könnte beispielsweise Hinweise auf die Ernährungsweise geben, während das Verhältnis von Flossenlänge zu Körpermasse (`flipper_body_ratio`) Rückschlüsse auf die Schwimmeffizienz zulässt. Die Multiplikation mit 1000 dient dabei lediglich der besseren Lesbarkeit der resultierenden Werte.

String-Manipulationen mit Funktionen wie `paste()` und `str_sub()` ermöglichen die Erstellung kombinierter Identifikatoren oder abgekürzter Versionen bestehender Variablen. Dies ist besonders nützlich für Visualisierungen, wo Platzbeschränkungen eine Rolle spielen, oder für die Erstellung eindeutiger Gruppierungsschlüssel.

Standardisierung und Transformation sind fundamentale Schritte in der statistischen Analyse. Die Konversion von Gramm zu Kilogramm (`body_mass_kg`) verbessert die Interpretierbarkeit, während die Z-Score-Transformation (`body_mass_scaled`) die Werte auf Mittelwert 0 und Standardabweichung 1 normiert. Dies ist besonders wichtig für Machine Learning-Algorithmen, die empfindlich auf unterschiedliche Wertebereiche reagieren, und für vergleichende statistische Analysen.

Das Ranking mit `dense_rank()` erstellt eine Rangfolge der Pinguine nach Körpermasse, wobei `desc()` für absteigende Sortierung sorgt und `dense_rank()` im Gegensatz zu `rank()` bei Gleichständen keine Ränge überspringt. Diese neue Variable kann später für die Identifikation extremer Werte oder für gruppierte Analysen der schwersten/leichtesten Individuen pro Art oder Insel verwendet werden.
```{r}
penguins_advanced <- penguins %>%
  mutate(
    # CONDITIONAL LOGIC mit case_when()
    # Syntax: case_when(Bedingung ~ Ergebnis, TRUE ~ Default)
    size_category = case_when(
      body_mass_g < 3000 ~ "Small",      # Wenn Masse < 3000g
      body_mass_g < 4500 ~ "Medium",     # Sonst wenn < 4500g  
      body_mass_g < 6000 ~ "Large",      # Sonst wenn < 6000g
      TRUE ~ "Extra Large"               # Alle anderen Fälle
    ),
    
    # NUMERISCHE BERECHNUNGEN
    # Ratio-Berechnungen helfen, Proportionen zwischen Körperteilen zu verstehen
    bill_ratio = bill_length_mm / bill_depth_mm,
    flipper_body_ratio = flipper_length_mm / body_mass_g * 1000,  # *1000 für bessere Lesbarkeit
    
    # STRING-MANIPULATION
    # paste() verbindet Strings, str_sub() extrahiert Teilstrings
    species_island = paste(species, island, sep = "_"),
    species_short = str_sub(species, 1, 3),  # Erste 3 Buchstaben
    
    # STANDARDISIERUNG UND TRANSFORMATION
    body_mass_kg = body_mass_g / 1000,                    # Einheitenkonversion
    body_mass_scaled = as.vector(scale(body_mass_g)),     # Z-Score (Mittelwert=0, SD=1)
    
    # RANKING innerhalb von Gruppen (wird später nützlich)
    mass_rank = dense_rank(desc(body_mass_g))  # 1 = schwerster Pinguin
  )

# Schaue dir die neuen Spalten an
# mit select() kann man ein subset eines dataframes auswählen
penguins_advanced %>% 
  select(species, body_mass_g, size_category, bill_ratio, body_mass_scaled) %>%
  head(10)
```
WARUM das nützlich ist:

- size_category: Kategorisierung für Gruppierungen und Plots
- Ratios: Wichtig für allometrische Analysen in der Biologie
- Scaling: Nötig für Machine Learning und statistische Analysen

## 1.2 Fortgeschrittenes Grouping und Summarizing

### Gruppierte deskriptive Statistiken mit `group_by()` und `summarise()`

Die Kombination aus `group_by()` und `summarise()` ist das Herzstück der gruppierten Datenanalyse im tidyverse. Während `group_by()` den Datensatz in Untergruppen aufteilt (hier nach jeder Kombination aus Pinguinart und Insel), berechnet `summarise()` für jede dieser Gruppen separate statistische Kennzahlen und erzeugt dabei ein **neues, kompaktes Dataframe**. 

Im Gegensatz zu `mutate()`, das neue Spalten zum bestehenden Datensatz hinzufügt und alle ursprünglichen Zeilen beibehält, reduziert `summarise()` die Datenstruktur drastisch: Aus den ursprünglich 344 Pinguinbeobachtungen entstehen nur noch wenige Zeilen – eine pro Gruppe (Art-Insel-Kombination) – mit den berechneten Zusammenfassungen als neue Spalten.

Der Code berechnet zunächst grundlegende deskriptive Statistiken wie Mittelwerte, Mediane und Standardabweichungen für jede Gruppe. Besonders wichtig ist dabei der Parameter `na.rm = TRUE`, der fehlende Werte aus den Berechnungen ausschließt. Zusätzlich werden Quantile (25% und 75%) berechnet, die später für die Bestimmung des Interquartilsabstands verwendet werden und wichtige Informationen über die Verteilung der Daten liefern.

Die Anteilsberechnung `prop_female` zeigt eine clevere Anwendung logischer Operationen: `sum(sex == "female", na.rm = TRUE)` zählt alle TRUE-Werte (also alle Weibchen), während `sum(!is.na(sex))` die Anzahl aller Beobachtungen mit bekanntem Geschlecht ermittelt. Der Parameter `.groups = "drop"` am Ende des `summarise()`-Blocks ist wichtig, da er die Gruppierung wieder aufhebt und verhindert, dass nachfolgende Operationen unbeabsichtigt noch gruppiert ausgeführt werden.

Im anschließenden `mutate()`-Schritt werden aus den bereits berechneten Kennzahlen weitere aussagekräftige Variablen abgeleitet: Der Interquartilsabstand (`mass_range`) als Maß für die Streuung innerhalb jeder Gruppe, ein Variationskoeffizient zur relativen Einordnung der Variabilität und eine kategorische Einteilung der Stichprobengrößen. Das finale `arrange()` und `select()` sorgt für eine übersichtliche Darstellung der wichtigsten Ergebnisse, sortiert nach dem durchschnittlichen Körpergewicht der Gruppen.

Diese Analysestrategie ist fundamental für die explorative Datenanalyse, da sie systematische Unterschiede zwischen Subpopulationen aufdeckt und die Basis für weiterführende statistische Tests oder Visualisierungen bildet.

```{r}
# Gruppierte Statistik: Nach Art UND Insel zusammenfassen
penguin_stats <- penguins %>%
  group_by(species, island) %>%    # Gruppierung: Jede Kombination aus Art & Insel ist eine Gruppe
  summarise(
    # Basisstatistiken
    count = n(),                        # Anzahl Beobachtungen in der Gruppe
    mean_body_mass = mean(body_mass_g, na.rm = TRUE),    # Durchschnittliches Körpergewicht (ignoriert NA)
    median_flipper = median(flipper_length_mm, na.rm = TRUE), # Median Flossenlänge (ignoriert NA)
    sd_bill_length = sd(bill_length_mm, na.rm = TRUE),   # Standardabweichung Schnabellänge (ignoriert NA)
    
    # Quantile: Nützlich für Boxplots und Ausreißer-Erkennung
    q25_mass = quantile(body_mass_g, 0.25, na.rm = TRUE), # 25%-Quantil (unteres Viertel)
    q75_mass = quantile(body_mass_g, 0.75, na.rm = TRUE), # 75%-Quantil (oberes Viertel)
    
    # Minimum / Maximum
    min_bill_depth = min(bill_depth_mm, na.rm = TRUE),    # Kleinster Schnabel-Tiefenwert
    max_bill_depth = max(bill_depth_mm, na.rm = TRUE),    # Größter Schnabel-Tiefenwert
    
    # Anteil Weibchen (TRUE = weiblich, NA werden ignoriert)
    prop_female = sum(sex == "female", na.rm = TRUE) / sum(!is.na(sex)),
    
    # Weitere Statistik: Varianz des Körpergewichts
    var_mass = var(body_mass_g, na.rm = TRUE),           # Streuung des Gewichts
    
    .groups = "drop"      # Gruppierung nach dem summarise entfernen (damit Folgefunktionen normal arbeiten)
  ) %>%
  # Neue Variablen aus Ergebnissen berechnen
  mutate(
    mass_range = q75_mass - q25_mass,              # Interquartilsabstand = mittlere Streuung des Gewichts
    cv_bill_length = sd_bill_length / mean_body_mass,  # Variationskoeffizient: relative Streuung Schnabellänge
    sample_size_category = case_when(              # Gruppengröße klassifizieren
      count < 50 ~ "Small sample",
      count < 100 ~ "Medium sample", 
      TRUE ~ "Large sample"
    )
  )

# Ergebnisse ansehen: Sortieren nach mittlerem Gewicht, nur wichtige Spalten anzeigen
penguin_stats %>%
  arrange(desc(mean_body_mass)) %>%    # Größte Durchschnittsgewichte zuerst zeigen
  select(species, island, count, mean_body_mass, mass_range, prop_female)

# INTERPRETATION der Ergebnisse:
print("Interpretationshilfen:")
print("- Gentoo-Pinguine sind im Durchschnitt am schwersten")
print("- mass_range zeigt die Variabilität innerhalb jeder Art/Insel-Kombination")
print("- prop_female zeigt Geschlechterverteilung (sollte ~0.5 sein)")

```


WARUM das wichtig ist:

- Deskriptive Statistik ist Grundlage jeder Datenanalyse
- Gruppierte Analysen decken Unterschiede zwischen Subpopulationen auf
- Diese Kennzahlen sind Basis für statistische Tests


## 1.3 Erweiterte Filter- und Slice-Operationen

### Präzise Datenauswahl mit erweiterten Filter- und Slice-Operationen

Die gezielte Auswahl relevanter Datensubsets ist ein fundamentaler Schritt in jeder Datenanalyse. Während `filter()` zeilenbasiert auf logischen Bedingungen operiert, arbeitet `slice()` positionsbasiert und ermöglicht die Auswahl spezifischer Zeilen innerhalb von Gruppen. Diese beiden Ansätze ergänzen sich perfekt für komplexe Auswahlkriterien.

Die erweiterte Filteroperation im Code demonstriert verschiedene Filterstrategien, die in der Praxis häufig kombiniert werden. Die Verwendung von `!is.na(sex)` entfernt Beobachtungen mit fehlenden Geschlechtsangaben – eine wichtige Datenbereinigung, da fehlende Werte spätere Analysen beeinträchtigen können. Der numerische Filter `body_mass_g > 3000` fokussiert die Analyse auf größere Pinguine, was je nach Forschungsfrage sinnvoll sein kann.

Besonders mächtig ist die Kombination verschiedener Filtertypen: `species %in% c("Adelie", "Chinstrap")` nutzt den `%in%`-Operator für die Auswahl mehrerer Kategorien aus einer Liste, während `str_detect(island, "^T|^B")` mit regulären Ausdrücken arbeitet. Hier bedeutet `^T|^B` "beginnt mit T oder B", wobei das `^`-Symbol den Zeilenanfang markiert und `|` für "oder" steht. Die `between()`-Funktion bietet eine elegante Alternative zu `variable >= x & variable <= y` und verbessert die Lesbarkeit des Codes erheblich.

Die anschließende Dokumentation der Filtereffekte mit `nrow()` ist eine wichtige Praxis, um zu verstehen, wie stark die Datenauswahl das ursprüngliche Dataset reduziert. Diese Information hilft bei der Bewertung der Repräsentativität der gefilterten Daten und kann auf potenzielle Probleme mit zu restriktiven Filtern hinweisen.

Die `slice()`-Operationen erweitern die Auswahlmöglichkeiten um positionsbasierte Kriterien. `slice_max()` in Kombination mit `group_by()` ermöglicht es, die Top-N-Werte innerhalb jeder Gruppe zu identifizieren – hier die drei schwersten Pinguine jeder Art. Diese Technik ist besonders wertvoll für Ranking-Analysen und die Identifikation von Extremwerten innerhalb von Subgruppen. Das nachfolgende `ungroup()` ist wichtig, um sicherzustellen, dass spätere Operationen wieder auf den gesamten Datensatz wirken.

Die Zufallsstichprobe mit `slice_sample()` ist ein mächtiges Werkzeug für große Datensätze, bei denen eine Vollanalyse unpraktikabel wäre. `set.seed(123)` gewährleistet dabei die Reproduzierbarkeit der Zufallsauswahl – ein kritischer Aspekt für wissenschaftliche Analysen. Die gruppierte Stichprobenziehung sorgt dafür, dass aus jeder Art die gleiche Anzahl Beobachtungen gezogen wird, was wichtig für ausgewogene Analysen ist.

Die verschiedenen Slice-Varianten (`slice_head()`, `slice_tail()`, `slice_min()`) bieten flexible Möglichkeiten für unterschiedliche Auswahlszenarien. Während `slice_head()` und `slice_tail()` die ersten oder letzten Zeilen auswählen (nützlich für zeitlich sortierte Daten), ermöglicht `slice_min()` die Auswahl der kleinsten Werte einer Variablen. Die proportionale Stichprobe mit `slice_sample(prop = 0.1)` ist besonders praktisch, wenn ein fester Prozentsatz des Datensatzes benötigt wird, unabhängig von der absoluten Größe.

Diese Techniken sind essentiell für explorative Datenanalysen, da sie schnelle Einblicke in Datenverteilungen und Extremwerte ermöglichen, ohne den gesamten Datensatz verarbeiten zu müssen. Gleichzeitig bilden sie die Grundlage für komplexere Analysestrategien und Hypothesentests.

```{r}
# MEHRFACH-FILTER mit verschiedenen Datentypen
filtered_penguins <- penguins %>%
  filter(
    # LOGISCHE OPERATOREN
    !is.na(sex),                      # Entferne Zeilen ohne Geschlecht
    body_mass_g > 3000,              # Nur "größere" Pinguine
    
    # STRING-PATTERN mit %in%
    species %in% c("Adelie", "Chinstrap"),  # Nur diese beiden Arten
    
    # REGEX mit str_detect()
    str_detect(island, "^T|^B"),     # Inseln, die mit T oder B beginnen
    
    # NUMERISCHE BEREICHE mit between()
    between(bill_length_mm, 35, 50)  # Schnabellänge zwischen 35-50mm
  )

# Wie viele Pinguine bleiben übrig?
cat("Original:", nrow(penguins), "Pinguine\n")
cat("Nach Filterung:", nrow(filtered_penguins), "Pinguine\n")
cat("Anteil behalten:", round(nrow(filtered_penguins)/nrow(penguins)*100, 1), "%\n")

# ========== Intelligente Slice-Operationen ==========

# TOP-N AUSWAHL pro Gruppe
top_penguins <- penguins %>%
  filter(!is.na(body_mass_g)) %>%           # Entferne fehlende Werte
  group_by(species) %>%                     # Für jede Art separat
  slice_max(order_by = body_mass_g, n = 3) %>%  # Top 3 schwerste pro Art
  ungroup() %>%                            # Gruppierung aufheben
  arrange(species, desc(body_mass_g))      # Sortieren für bessere Übersicht

print("Die 3 schwersten Pinguine jeder Art:")
print(top_penguins %>% select(species, sex, body_mass_g, island))

# ZUFÄLLIGE STICHPROBE (wichtig für große Datasets)
set.seed(123)  # Für reproduzierbare Ergebnisse
sample_penguins <- penguins %>%
  group_by(species) %>%
  slice_sample(n = 5) %>%     # 5 zufällige pro Art
  ungroup()

print("Zufällige Stichprobe (5 pro Art):")
print(sample_penguins %>% count(species))

# WEITERE SLICE-VARIANTEN
# slice_head(n = 2)    - Erste 2 Zeilen pro Gruppe
# slice_tail(n = 2)    - Letzte 2 Zeilen pro Gruppe  
# slice_min(var, n=2)  - Kleinste 2 Werte einer Variable
# slice_sample(prop=0.1) - 10% zufällige Stichprobe
```

WARUM Slice-Operationen nützlich sind:

- Explorative Datenanalyse: Schneller Überblick über Extreme
- Stichprobenziehung: Reduziere große Datasets für Tests
- Ranking-Analysen: Finde Top-Performer, Ausreißer etc.
